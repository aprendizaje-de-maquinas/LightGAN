This is an implementation of what we call a LightGAN. A GAN using the LightRNN structure from Microsoft Research to implement language modeling at a large scale. 

The data that is used for this project is from the SNAP lab at Stanford and is not publically available so we cannot post the data that we used.

Created for CS230 at Stanford.

To Stanford students: do not use this code for Stanford projects without proper attributation as that would be a breach of the honor code. 

References:
J. Yang, J. Leskovec. Temporal Variation in Online Media. ACM International Conference on Web Search and Data Mining (WSDM '11), 2011.
    
J. Leskovec, A. Krevl. SNAP Datasets: Stanford Large Network Dataset Collection, June 2014.
    
Xiang Li, Tao Qin, Jian Yang, Xiaolin Hu, Tie-Yan Liu: LightRNN: Memory and Computation-Efficient Recurrent Neural Networks. NIPS 2016: 4385-4393

Ofir Press, Amir Bar, Ben Bogin, Jonathan Berant, Lior Wolf: Language Generation with Recurrent Generative Adversarial Networks without Pre-training. CoRR abs/1706.01399 (2017)

Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin and Aaron C. Courville. Improved Training of Wasserstein GANs. CoRR abs/1704.00028 (2017)
